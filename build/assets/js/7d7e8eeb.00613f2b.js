"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[294],{2959(e,n,o){o.r(n),o.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>s,metadata:()=>a,toc:()=>c});var i=o(4848),t=o(8453);const s={title:"Connecting AI Agents to Robots",sidebar_label:"Chapter 2 - AI Agents & ROS2",description:"Understanding how AI agents connect to robots using rclpy, bridging decision logic to robot controllers, and the perception-decision-actuation flow"},r="Connecting AI Agents to Robots",a={id:"modules/physical-ai-robotics/module-1-ros2-nervous-system/chapter-2-ai-agents-ros2",title:"Connecting AI Agents to Robots",description:"Understanding how AI agents connect to robots using rclpy, bridging decision logic to robot controllers, and the perception-decision-actuation flow",source:"@site/docs/modules/physical-ai-robotics/module-1-ros2-nervous-system/chapter-2-ai-agents-ros2.md",sourceDirName:"modules/physical-ai-robotics/module-1-ros2-nervous-system",slug:"/modules/physical-ai-robotics/module-1-ros2-nervous-system/chapter-2-ai-agents-ros2",permalink:"/physical-ai-humanoid-robotics/docs/modules/physical-ai-robotics/module-1-ros2-nervous-system/chapter-2-ai-agents-ros2",draft:!1,unlisted:!1,editUrl:"https://github.com/your-username/physical-ai-humanoid-robotics/tree/main/docs/modules/physical-ai-robotics/module-1-ros2-nervous-system/chapter-2-ai-agents-ros2.md",tags:[],version:"current",frontMatter:{title:"Connecting AI Agents to Robots",sidebar_label:"Chapter 2 - AI Agents & ROS2",description:"Understanding how AI agents connect to robots using rclpy, bridging decision logic to robot controllers, and the perception-decision-actuation flow"},sidebar:"tutorialSidebar",previous:{title:"Chapter 1: ROS 2 Fundamentals \u2013 The Nervous System",permalink:"/physical-ai-humanoid-robotics/docs/modules/physical-ai-robotics/module-1-ros2-nervous-system/chapter-1-ros2-fundamentals"},next:{title:"Chapter 3 - Robot Anatomy & URDF",permalink:"/physical-ai-humanoid-robotics/docs/modules/physical-ai-robotics/module-1-ros2-nervous-system/chapter-3-robot-anatomy-urdf"}},l={},c=[{value:"Introduction to rclpy and ROS 2 Nodes",id:"introduction-to-rclpy-and-ros-2-nodes",level:2},{value:"Key Benefits of rclpy for AI Integration",id:"key-benefits-of-rclpy-for-ai-integration",level:3},{value:"Bridging AI Decision Logic to Robot Controllers",id:"bridging-ai-decision-logic-to-robot-controllers",level:2},{value:"The Bridge Architecture",id:"the-bridge-architecture",level:3},{value:"Key Components of the Bridge:",id:"key-components-of-the-bridge",level:4},{value:"Command Flow: Perception \u2192 Decision \u2192 Actuation",id:"command-flow-perception--decision--actuation",level:2},{value:"1. Perception Phase",id:"1-perception-phase",level:3},{value:"2. Decision Phase",id:"2-decision-phase",level:3},{value:"3. Actuation Phase",id:"3-actuation-phase",level:3},{value:"Real-World Applications in Humanoid Control",id:"real-world-applications-in-humanoid-control",level:2},{value:"Autonomous Navigation",id:"autonomous-navigation",level:3},{value:"Object Manipulation",id:"object-manipulation",level:3},{value:"Human-Robot Interaction",id:"human-robot-interaction",level:3},{value:"Conceptual Example Scenarios for Humanoid Behaviors",id:"conceptual-example-scenarios-for-humanoid-behaviors",level:2},{value:"Scenario 1: Humanoid Navigation in Dynamic Environments",id:"scenario-1-humanoid-navigation-in-dynamic-environments",level:3},{value:"Scenario 2: Object Manipulation and Grasping",id:"scenario-2-object-manipulation-and-grasping",level:3},{value:"Scenario 3: Human-Robot Collaboration",id:"scenario-3-human-robot-collaboration",level:3},{value:"Practical Implementation Example: Mobile Robot Control",id:"practical-implementation-example-mobile-robot-control",level:2},{value:"Advanced Example: Humanoid Upper Body Control",id:"advanced-example-humanoid-upper-body-control",level:3},{value:"Best Practices for AI-Agent Integration",id:"best-practices-for-ai-agent-integration",level:2},{value:"Cross-References to Related Topics",id:"cross-references-to-related-topics",level:2},{value:"Summary",id:"summary",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Exercises",id:"exercises",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h1,{id:"connecting-ai-agents-to-robots",children:"Connecting AI Agents to Robots"}),"\n",(0,i.jsx)(n.h2,{id:"introduction-to-rclpy-and-ros-2-nodes",children:"Introduction to rclpy and ROS 2 Nodes"}),"\n",(0,i.jsx)(n.p,{children:"ROS 2 provides Python clients through rclpy, enabling AI agents to interface with robotic hardware. The rclpy library bridges high-level AI decision-making algorithms and low-level robot control systems."}),"\n",(0,i.jsx)(n.h3,{id:"key-benefits-of-rclpy-for-ai-integration",children:"Key Benefits of rclpy for AI Integration"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Language Compatibility"}),": Python is the dominant language for AI and ML development"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Direct Integration"}),": Native communication with ROS 2 middleware"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Real-time Performance"}),": Efficient message passing between AI algorithms and robot controllers"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"bridging-ai-decision-logic-to-robot-controllers",children:"Bridging AI Decision Logic to Robot Controllers"}),"\n",(0,i.jsx)(n.p,{children:"The connection between AI agents and robots occurs through ROS 2 nodes that act as intermediaries. These nodes translate high-level goals from AI systems into actionable commands for robot hardware."}),"\n",(0,i.jsx)(n.h3,{id:"the-bridge-architecture",children:"The Bridge Architecture"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"AI Agent (High-Level Goals) \u2192 ROS 2 Bridge Node \u2192 Robot Controllers (Low-Level Actions)\n"})}),"\n",(0,i.jsx)(n.h4,{id:"key-components-of-the-bridge",children:"Key Components of the Bridge:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Goal Translation Layer"}),": Converts abstract AI decisions into specific robot commands"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Safety Validation"}),": Ensures AI commands comply with safety constraints"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"State Monitoring"}),": Tracks robot state and reports back to AI agent"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Fallback Mechanisms"}),": Handles communication failures gracefully"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"command-flow-perception--decision--actuation",children:"Command Flow: Perception \u2192 Decision \u2192 Actuation"}),"\n",(0,i.jsx)(n.p,{children:"The fundamental cycle of AI-driven robotics operates in a continuous loop:"}),"\n",(0,i.jsx)(n.h3,{id:"1-perception-phase",children:"1. Perception Phase"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Sensors collect environmental data (cameras, lidars, IMUs, joint encoders)"}),"\n",(0,i.jsx)(n.li,{children:"Data is published to ROS 2 topics for AI consumption"}),"\n",(0,i.jsx)(n.li,{children:"Sensor fusion combines multiple data streams into coherent world models"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"2-decision-phase",children:"2. Decision Phase"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"AI agents consume sensor data from ROS 2 topics"}),"\n",(0,i.jsx)(n.li,{children:"Machine learning models process information to make decisions"}),"\n",(0,i.jsx)(n.li,{children:"Decisions are packaged as ROS 2 service calls or topic messages"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"3-actuation-phase",children:"3. Actuation Phase"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Robot controllers receive AI-generated commands via ROS 2"}),"\n",(0,i.jsx)(n.li,{children:"Commands are translated into motor commands, gripper controls, etc."}),"\n",(0,i.jsx)(n.li,{children:"Robot executes actions and publishes feedback to the AI system"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"real-world-applications-in-humanoid-control",children:"Real-World Applications in Humanoid Control"}),"\n",(0,i.jsx)(n.h3,{id:"autonomous-navigation",children:"Autonomous Navigation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Camera/Lidar Data \u2192 AI Path Planning \u2192 Leg Motor Commands\n"})}),"\n",(0,i.jsx)(n.h3,{id:"object-manipulation",children:"Object Manipulation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Vision Processing \u2192 Grasping Strategy \u2192 Arm Joint Control\n"})}),"\n",(0,i.jsx)(n.h3,{id:"human-robot-interaction",children:"Human-Robot Interaction"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Speech Recognition \u2192 Dialogue Manager \u2192 Expressive Behaviors\n"})}),"\n",(0,i.jsx)(n.h2,{id:"conceptual-example-scenarios-for-humanoid-behaviors",children:"Conceptual Example Scenarios for Humanoid Behaviors"}),"\n",(0,i.jsx)(n.h3,{id:"scenario-1-humanoid-navigation-in-dynamic-environments",children:"Scenario 1: Humanoid Navigation in Dynamic Environments"}),"\n",(0,i.jsx)(n.p,{children:"Consider a humanoid robot that needs to navigate through a crowded space:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Perception: Vision and depth sensors detect humans moving in the environment\nDecision: AI agent determines safest path around people using social navigation algorithms\nActuation: Leg controllers execute walking gait to follow planned path while avoiding collisions\n"})}),"\n",(0,i.jsx)(n.h3,{id:"scenario-2-object-manipulation-and-grasping",children:"Scenario 2: Object Manipulation and Grasping"}),"\n",(0,i.jsx)(n.p,{children:"For a humanoid robot picking up objects:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Perception: RGB-D camera and tactile sensors detect object position and properties\nDecision: AI agent plans grasp strategy based on object shape, weight, and task requirements\nActuation: Arm and hand controllers execute precise joint movements to grasp and manipulate\n"})}),"\n",(0,i.jsx)(n.h3,{id:"scenario-3-human-robot-collaboration",children:"Scenario 3: Human-Robot Collaboration"}),"\n",(0,i.jsx)(n.p,{children:"In a collaborative task scenario:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Perception: Microphones and cameras detect human speech and gestures\nDecision: AI agent interprets human intent and determines appropriate response\nActuation: Full-body controllers execute movements that are safe and socially appropriate\n"})}),"\n",(0,i.jsx)(n.h2,{id:"practical-implementation-example-mobile-robot-control",children:"Practical Implementation Example: Mobile Robot Control"}),"\n",(0,i.jsx)(n.p,{children:"Here's a simplified example of how an AI agent might control a mobile robot, which can be extended to humanoid systems:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Twist\nfrom sensor_msgs.msg import LaserScan\n\nclass AIAgentController(Node):\n    def __init__(self):\n        super().__init__('ai_agent_controller')\n\n        # Publisher for robot velocity commands\n        self.cmd_vel_publisher = self.create_publisher(Twist, '/cmd_vel', 10)\n\n        # Subscriber for laser scan data\n        self.scan_subscriber = self.create_subscription(\n            LaserScan,\n            '/scan',\n            self.scan_callback,\n            10\n        )\n\n        # Timer for AI decision-making loop\n        self.timer = self.create_timer(0.1, self.ai_decision_loop)\n\n    def scan_callback(self, msg):\n        # Process laser scan data for obstacle detection\n        self.laser_data = msg\n\n    def ai_decision_loop(self):\n        # Simple AI logic: avoid obstacles and move forward\n        cmd_msg = Twist()\n\n        if self.is_obstacle_ahead():\n            # Turn to avoid obstacle\n            cmd_msg.angular.z = 0.5\n        else:\n            # Move forward\n            cmd_msg.linear.x = 0.5\n\n        self.cmd_vel_publisher.publish(cmd_msg)\n\n    def is_obstacle_ahead(self):\n        if hasattr(self, 'laser_data'):\n            # Check if there are obstacles within 1 meter in front\n            front_distances = self.laser_data.ranges[:10] + self.laser_data.ranges[-10:]\n            return min(front_distances) < 1.0\n        return False\n\ndef main():\n    rclpy.init()\n    ai_controller = AIAgentController()\n\n    try:\n        rclpy.spin(ai_controller)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        ai_controller.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.h3,{id:"advanced-example-humanoid-upper-body-control",children:"Advanced Example: Humanoid Upper Body Control"}),"\n",(0,i.jsx)(n.p,{children:"Here's a more complex example showing how an AI agent might control a humanoid robot's upper body for object manipulation:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import JointState\nfrom trajectory_msgs.msg import JointTrajectory, JointTrajectoryPoint\nfrom geometry_msgs.msg import PoseStamped\nimport numpy as np\n\nclass HumanoidUpperBodyController(Node):\n    def __init__(self):\n        super().__init__('humanoid_upper_body_controller')\n\n        # Publisher for joint trajectory commands\n        self.joint_cmd_publisher = self.create_publisher(\n            JointTrajectory,\n            '/joint_trajectory',\n            10\n        )\n\n        # Subscriber for target poses from AI agent\n        self.target_subscriber = self.create_subscription(\n            PoseStamped,\n            '/target_pose',\n            self.target_callback,\n            10\n        )\n\n        # Current joint state subscriber\n        self.joint_state_subscriber = self.create_subscription(\n            JointState,\n            '/joint_states',\n            self.joint_state_callback,\n            10\n        )\n\n        self.current_joint_positions = {}\n        self.joint_names = ['left_shoulder_pitch', 'left_shoulder_roll',\n                           'left_elbow', 'right_shoulder_pitch',\n                           'right_shoulder_roll', 'right_elbow']\n\n    def joint_state_callback(self, msg):\n        \"\"\"Update current joint positions\"\"\"\n        for i, name in enumerate(msg.name):\n            if name in self.joint_names:\n                self.current_joint_positions[name] = msg.position[i]\n\n    def target_callback(self, msg):\n        \"\"\"Process target pose from AI agent and generate joint trajectory\"\"\"\n        # This would involve inverse kinematics to convert\n        # target pose to joint angles\n        target_joint_positions = self.inverse_kinematics(\n            msg.pose,\n            self.current_joint_positions\n        )\n\n        # Create and publish joint trajectory\n        trajectory_msg = self.create_trajectory_msg(target_joint_positions)\n        self.joint_cmd_publisher.publish(trajectory_msg)\n\n    def inverse_kinematics(self, target_pose, current_joints):\n        \"\"\"Simplified inverse kinematics calculation\"\"\"\n        # In a real implementation, this would use sophisticated\n        # IK algorithms or learning-based approaches\n        target_joints = current_joints.copy()\n\n        # Simplified example - in reality this would be complex math\n        target_joints['left_shoulder_pitch'] += 0.1\n        target_joints['left_elbow'] -= 0.2\n\n        return target_joints\n\n    def create_trajectory_msg(self, joint_positions):\n        \"\"\"Create a joint trajectory message\"\"\"\n        trajectory = JointTrajectory()\n        trajectory.joint_names = list(joint_positions.keys())\n\n        point = JointTrajectoryPoint()\n        point.positions = list(joint_positions.values())\n        point.time_from_start.sec = 2  # 2 seconds to reach target\n\n        trajectory.points.append(point)\n        return trajectory\n\ndef main():\n    rclpy.init()\n    controller = HumanoidUpperBodyController()\n\n    try:\n        rclpy.spin(controller)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        controller.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.h2,{id:"best-practices-for-ai-agent-integration",children:"Best Practices for AI-Agent Integration"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Modular Design"}),": Keep AI logic separate from ROS 2 communication"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Error Handling"}),": Implement robust error recovery mechanisms"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Performance Monitoring"}),": Track latency between perception and action"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Safety First"}),": Always validate AI commands before execution"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Logging"}),": Maintain detailed logs for debugging and analysis"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"cross-references-to-related-topics",children:"Cross-References to Related Topics"}),"\n",(0,i.jsx)(n.p,{children:"To deepen your understanding of AI-agent integration in humanoid robotics, consider these related topics:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ROS 2 Fundamentals"}),": Review ",(0,i.jsx)(n.a,{href:"/physical-ai-humanoid-robotics/docs/modules/physical-ai-robotics/module-1-ros2-nervous-system/chapter-1-ros2-fundamentals",children:"Chapter 1"})," for core concepts of nodes, topics, and services that enable AI integration"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Robot Modeling"}),": See ",(0,i.jsx)(n.a,{href:"/physical-ai-humanoid-robotics/docs/modules/physical-ai-robotics/module-1-ros2-nervous-system/chapter-3-robot-anatomy-urdf",children:"Chapter 3"})," for how URDF models provide the structural foundation for AI control of robot bodies"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Simulation"}),": Understanding robot models is essential for testing AI agents in simulation environments like Gazebo"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Control Theory"}),": The perception-decision-actuation cycle connects to classical control systems concepts"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.p,{children:"Connecting AI agents to robots through ROS 2 enables sophisticated autonomous behaviors. The rclpy library provides the essential bridge between Python-based AI systems and robot hardware, allowing for seamless integration of perception, decision-making, and actuation. Understanding this connection is crucial for developing intelligent robotic systems that can operate autonomously in complex environments."}),"\n",(0,i.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"rclpy serves as the bridge between AI agents and ROS 2 robot systems"}),"\n",(0,i.jsx)(n.li,{children:"The perception-decision-actuation cycle forms the foundation of AI-driven robotics"}),"\n",(0,i.jsx)(n.li,{children:"Proper safety and validation mechanisms are essential for reliable operation"}),"\n",(0,i.jsx)(n.li,{children:"Modular design principles help maintain clean separation of concerns"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Implementation Challenge"}),": Extend the example AI agent controller to include obstacle avoidance with multiple sensor inputs (camera and LIDAR)."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"System Architecture"}),": Design a ROS 2 node structure for an AI agent that needs to recognize objects, plan grasps, and control a humanoid robot's arms."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Safety Analysis"}),": Identify three potential safety risks when connecting AI agents to humanoid robots and propose mitigation strategies for each."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Previous"}),": ",(0,i.jsx)(n.a,{href:"/physical-ai-humanoid-robotics/docs/modules/physical-ai-robotics/module-1-ros2-nervous-system/chapter-1-ros2-fundamentals",children:"Chapter 1 - ROS 2 Fundamentals"})," | ",(0,i.jsx)(n.strong,{children:"Next"}),": ",(0,i.jsx)(n.a,{href:"/physical-ai-humanoid-robotics/docs/modules/physical-ai-robotics/module-1-ros2-nervous-system/chapter-3-robot-anatomy-urdf",children:"Chapter 3 - Robot Anatomy with URDF"})]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453(e,n,o){o.d(n,{R:()=>r,x:()=>a});var i=o(6540);const t={},s=i.createContext(t);function r(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);